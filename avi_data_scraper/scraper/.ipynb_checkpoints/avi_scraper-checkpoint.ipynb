{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avalanche Data Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have wanted to assemble some of the data from CAIC for awhile now, and have decided to use the Covid-19 situation of 2020 as an excuse to get going on this now that I have more free time.\n",
    "\n",
    "I am mostly concerned with what the last few years have looked like for Colorado in terms of avalanche danger and how that has corresoponded to injuries and/or fatalities. CAIC does a really good job reporting on avalanche conditions but unfortunately it is hard to find data from past reports assembed in one, easy-to-find location. I built a little scraper to pull the data from CAIC so that I can/ assemble it somewhere and play around with it a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anyone unfamiliar with CAIC. This is the Colorado Avalanche Information Center. They do the avalanche research and reports in Colorado. This data is used by CDOT and backcountry explorers. https://avalanche.state.co.us/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasts are split into 10 zones, identified by zone_id.\n",
    "\n",
    "    zone_0 = Steamboat & Flat Tops\n",
    "    zone_1 = Front Range\n",
    "    zone_2 = Vail & Summit County\n",
    "    zone_3 = Sawatch\n",
    "    zone_4 = Aspen\n",
    "    zone_5 = Gunnison\n",
    "    zone_6 = Grand Mesa\n",
    "    zone_7 = North San Juan\n",
    "    zone_8 = South San Juan\n",
    "    zone_9 = Sangre de Cristo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To run this, install lxml to parse html from CAIC's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "from helpers import string_to_datetime, find_dates, danger_to_int\n",
    "import time\n",
    "\n",
    "# Assign url and specify zone_id\n",
    "def getAviData(zone_id):\n",
    "    url = 'https://avalanche.state.co.us/caic/pub_bc_avo.php?zone_id=' + str(zone_id)\n",
    "    csv_file = \"zone_\" + str(zone_id) +\"_aviDanger\" + \".csv\"\n",
    "    with open(csv_file, \"a\") as file:\n",
    "        file.write(\"date, danger_below, danger_near, danger_above\" + \"\\n\")\n",
    "\n",
    "    # This code retrieves all the report_ids that have been archived\n",
    "    response = requests.get(url)\n",
    "    tree = lh.document_fromstring(response.content)\n",
    "    value_script = tree.xpath(\"//div[@id='avalanche-forecast']/div[8]/ul/li[1]/script/text()\")\n",
    "    reports = find_dates(value_script)\n",
    "\n",
    "    # Iterate over dates using nested loop year 2013 through 2020\n",
    "    # This can be adjusted if just looking for specific dates. The current data goes back to 2013 though\n",
    "    # This code builds the requests we are going to post\n",
    "    # I had to submit a request for each report so that I could load the page associated with the report and then\n",
    "    # parse the html from each page\n",
    "    years = ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
    "\n",
    "    for year in years:\n",
    "        for day in reports[year]['values']:\n",
    "            form_data = {\n",
    "                    '_qf__arc_form' : '',\n",
    "                    'arc_bc_avo_fx_sel[0]': year,\n",
    "                    'arc_bc_avo_fx_sel[1]': day,\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, data=form_data)\n",
    "\n",
    "            # Occasionally I would overload CAIC's server and get kicked off, so this bit of code \n",
    "            # gives their page a quick break if the page does not load\n",
    "\n",
    "            # if the request does not yield a 200 status code, wait 1 minute and try again\n",
    "            # this should keep the server from getting overwhelmed\n",
    "            while response.status_code != 200:\n",
    "                time.sleep(60)\n",
    "                response = requests.post(url, data=form_data)\n",
    "\n",
    "            # Sweet! So now we can get start parsing html\n",
    "            # GATHER DATA:\n",
    "\n",
    "            tree = lh.document_fromstring(response.content)\n",
    "\n",
    "            # Here we snag the date and time of the reports and use a \"string_to_datetime\" helper function to get \n",
    "            # the data in a form that will be more useful later. I used a sql friendly format in case I wanted to \n",
    "            # save my data in a personal database\n",
    "            date_time = string_to_datetime(tree.xpath(\"//div[@id='avalanche-forecast']/table[1]/thead/tr/td[1]/h2/text()\")[0])\n",
    "\n",
    "            # Snag the avi_danger rating\n",
    "            # avi_danger(above, near, and at below treeline) saved as list in that order\n",
    "            danger_above = danger_to_int(tree.xpath(\"//div[@id='avalanche-forecast']/table[1]/tbody/tr[1]/td[3]/strong/text()\")[0])\n",
    "            danger_near = danger_to_int(tree.xpath(\"//div[@id='avalanche-forecast']/table[1]/tbody/tr[1]/td[5]/strong/text()\")[0])\n",
    "            danger_below = danger_to_int(tree.xpath(\"//div[@id='avalanche-forecast']/table[1]/tbody/tr[3]/td[3]/strong/text()\")[0])\n",
    "\n",
    "            # save the data to a text file in the format ('2013-12-23 16:23:00', [3, 3, 2]) line by line\n",
    "            with open(csv_file, \"a\") as file:\n",
    "                file.write(str(date_time)+ \",\" + str(danger_below) + \",\" + str(danger_near) + \",\" + str(danger_above) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAviData(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Burials to Avalanche Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I can gather and save avalanche data I am ready to gather data concerning avalanche accidents related to the backcoutry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data for accidents over the past seven years I have saved in a csv file. The data is organized as follows:\n",
    "          \n",
    "          date, zone_id, activity, number caught, number buried, number killed\n",
    "          \n",
    "Activities are abreviated:\n",
    "\n",
    "    sm = snowmobiling\n",
    "    sk = skiing\n",
    "    sb = snowboarding\n",
    "    sc = snow cat\n",
    "    ss = snow shoe\n",
    "    ft = on foot (such as in climbing)\n",
    "    ** = other such as shoveling ones roof off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2017-02-14 zone id: 0 activity: sm caught: 2 burried: 1 killed: 1\n",
      "date: 2017-01-12 zone id: 0 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-12-15 zone id: 0 activity: sb caught: 2 burried: 1 killed: 0\n",
      "date: 2016-12-11 zone id: 0 activity: sm caught: 1 burried: 2 killed: 0\n",
      "date: 2013-12-31 zone id: 0 activity: sb caught: 1 burried: 1 killed: 1\n",
      "date: 2019-12-08 zone id: 0 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2020-01-22 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2018-10-15 zone id: 1 activity: ft caught: 1 burried: 0 killed: 0\n",
      "date: 2013-11-21 zone id: 1 activity: ft caught: 1 burried: 0 killed: 0\n",
      "date: 2020-01-19 zone id: 1 activity: ft caught: 1 burried: 0 killed: 0\n",
      "date: 2013-11-24 zone id: 1 activity: ft caught: 1 burried: 0 killed: 0\n",
      "date: 2018-02-11 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2016-02-21 zone id: 1 activity: sb caught: 1 burried: 0 killed: 0\n",
      "date: 2018-12-18 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-03-07 zone id: 1 activity: sk caught: 2 burried: 1 killed: 1\n",
      "date: 2020-01-24 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2016-01-22 zone id: 1 activity: sb caught: 0 burried: 0 killed: 1\n",
      "date: 2020-02-10 zone id: 1 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2016-01-25 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2013-12-27 zone id: 1 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2014-12-31 zone id: 1 activity: ss caught: 1 burried: 1 killed: 1\n",
      "date: 2019-03-23 zone id: 1 activity: ft caught: 4 burried: 0 killed: 0\n",
      "date: 2020-02-15 zone id: 2 activity: sm caught: 3 burried: 2 killed: 2\n",
      "date: 2015-12-19 zone id: 2 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2015-11-21 zone id: 2 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-11-28 zone id: 2 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2014-01-02 zone id: 2 activity: sk caught: 6 burried: 0 killed: 0\n",
      "date: 2020-04-10 zone id: 2 activity: sm caught: 1 burried: 1 killed: 1\n",
      "date: 2019-03-07 zone id: 2 activity: vh caught: 5 burried: 0 killed: 0\n",
      "date: 2018-02-08 zone id: 2 activity: sb caught: 1 burried: 0 killed: 0\n",
      "date: 2016-03-19 zone id: 2 activity: sm caught: 1 burried: 0 killed: 0\n",
      "date: 2014-02-10 zone id: 2 activity: sk caught: 2 burried: 1 killed: 1\n",
      "date: 2016-03-21 zone id: 2 activity: ft caught: 1 burried: 0 killed: 0\n",
      "date: 2014-01-07 zone id: 2 activity: sk caught: 4 burried: 0 killed: 1\n",
      "date: 2015-05-31 zone id: 2 activity: sb caught: 1 burried: 0 killed: 0\n",
      "date: 2016-03-19 zone id: 2 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2015-03-05 zone id: 2 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2013-12-22 zone id: 2 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2016-01-22 zone id: 3 activity: sk caught: 1 burried: 1 killed: 0\n",
      "date: 2018-01-14 zone id: 3 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2016-02-05 zone id: 3 activity: sm caught: 1 burried: 0 killed: 1\n",
      "date: 2019-01-17 zone id: 3 activity: sm caught: 1 burried: 1 killed: 0\n",
      "date: 2014-02-15 zone id: 3 activity: sk caught: 5 burried: 3 killed: 2\n",
      "date: 2016-01-16 zone id: 3 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2020-02-11 zone id: 3 activity: sm caught: 1 burried: 0 killed: 0\n",
      "date: 2018-03-07 zone id: 3 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2018-12-14 zone id: 3 activity: sk caught: 0 burried: 0 killed: 0\n",
      "date: 2018-11-02 zone id: 3 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2015-01-01 zone id: 4 activity: sc caught: 1 burried: 0 killed: 0\n",
      "date: 2018-04-08 zone id: 4 activity: sk caught: 2 burried: 0 killed: 1\n",
      "date: 2018-11-24 zone id: 4 activity: sk caught: 0 burried: 1 killed: 0\n",
      "date: 2015-02-23 zone id: 4 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2015-05-03 zone id: 4 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2015-03-17 zone id: 4 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-01-11 zone id: 4 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2017-11-18 zone id: 4 activity: sb caught: 1 burried: 0 killed: 0\n",
      "date: 2019-01-21 zone id: 4 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2015-05-17 zone id: 4 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2019-03-09 zone id: 5 activity: ** caught: 0 burried: 2 killed: 1\n",
      "date: 2019-03-08 zone id: 5 activity: ** caught: 0 burried: 1 killed: 0\n",
      "date: 2018-02-19 zone id: 5 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2019-12-01 zone id: 5 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2014-02-10 zone id: 5 activity: sm caught: 2 burried: 1 killed: 1\n",
      "date: 2018-11-25 zone id: 5 activity: sk caught: 0 burried: 0 killed: 0\n",
      "date: 2016-01-21 zone id: 5 activity: sm caught: 2 burried: 1 killed: 1\n",
      "date: 2018-12-06 zone id: 5 activity: sk caught: 3 burried: 0 killed: 0\n",
      "date: 2019-02-16 zone id: 5 activity: sk caught: 2 burried: 2 killed: 2\n",
      "date: 2014-11-03 zone id: 5 activity: ft caught: 2 burried: 0 killed: 0\n",
      "date: 2014-12-18 zone id: 5 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2017-02-03 zone id: 5 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2016-12-12 zone id: 5 activity: sm caught: 1 burried: 1 killed: 0\n",
      "date: 2018-12-16 zone id: 7 activity: sm caught: 2 burried: 0 killed: 0\n",
      "date: 2020-03-31 zone id: 7 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2015-01-06 zone id: 7 activity: sk caught: 1 burried: 0 killed: 1\n",
      "date: 2015-11-25 zone id: 7 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2014-03-16 zone id: 7 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-01-21 zone id: 7 activity: sk caught: 0 burried: 1 killed: 0\n",
      "date: 2019-03-03 zone id: 7 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2020-03-24 zone id: 7 activity: sb caught: 1 burried: 0 killed: 0\n",
      "date: 2020-03-20 zone id: 7 activity: sb caught: 2 burried: 0 killed: 0\n",
      "date: 2018-01-21 zone id: 7 activity: sk caught: 2 burried: 0 killed: 1\n",
      "date: 2016-02-13 zone id: 7 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2016-01-02 zone id: 7 activity: sk caught: 1 burried: 0 killed: 0\n",
      "date: 2019-11-30 zone id: 7 activity: ss caught: 2 burried: 0 killed: 0\n",
      "date: 2019-01-03 zone id: 7 activity: ss caught: 3 burried: 0 killed: 0\n",
      "date: 2016-04-03 zone id: 7 activity: sk caught: 2 burried: 0 killed: 0\n",
      "date: 2019-02-19 zone id: 7 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2019-03-12 zone id: 7 activity: ** caught: 3 burried: 0 killed: 0\n",
      "date: 2019-01-05 zone id: 7 activity: sk caught: 6 burried: 1 killed: 1\n",
      "date: 2020-01-18 zone id: 7 activity: ft caught: 1 burried: 1 killed: 1\n",
      "date: 2018-02-20 zone id: 7 activity: sb caught: 2 burried: 1 killed: 0\n",
      "date: 2014-03-04 zone id: 8 activity: sk caught: 1 burried: 1 killed: 1\n",
      "date: 2016-02-02 zone id: 8 activity: sm caught: 2 burried: 1 killed: 1\n",
      "date: 2014-03-05 zone id: 8 activity: sm caught: 1 burried: 1 killed: 1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"accidents_.csv\", newline = \"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        print(\"date: \" + row['\\ufeffdate'],\n",
    "              \"zone id: \" + row['zone_id'],\n",
    "              \"activity: \" + row['activity'], \n",
    "              \"caught: \" + row['caught'],\n",
    "              \"burried: \" + row['buried'],\n",
    "              \"killed: \" + row['killed'])           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone 1: Steamboat & Flat Tops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
